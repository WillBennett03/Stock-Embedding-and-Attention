# -*- coding: utf-8 -*-
"""Stockembedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nYvW2S-XeT36M81ZWTXqifvZ057NCji3
"""

import tensorflow as tf
import pandas_datareader as web
import numpy as np
import matplotlib.pyplot as plt
import time 
import pandas as pd
import pickle

table=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
SP500 = list(table[0]['Symbol'])

Stock_list = [
              'AMZN', 'AAPL', 'GOOG', 'GOOGL', 'MMM', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADBE', 'AMD', 'AAP'
              # 'AES', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALXN', 'ALGN', 'ALLE', 'LNT', 'ALL', 'MO',
              # 'AMCR', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC'
              ]
# Stock_list = SP500
Stock_index = dict(zip(Stock_list, range(len(Stock_list))))

def get_stock(symbol):
  try:
    return web.DataReader(symbol, data_source='quandl', api_key='Qm5z1gSsZaN_yhXmSCfi')
  except:                           
    return False

embedding_size = 15
output_size = 5
Stock_len = len(Stock_list)
print(Stock_list)

print(tf.__version__)

# Preprocessing

def stock_clean(stock_list):
  Min_date = None
  Max_date = 0
  stock_df_list = []
  Stock_tokens = []
  Time_tokens = []
  Stock_data = []
  Change = None
  failed_index = []
  
  for index, stock in enumerate(stock_list):
    print(stock)
    
    Stock_tokens.append(index)
    #get data and reverse
    df = get_stock(stock)
    if type(df) == type(False):
      failed_index.append(index)
    else:
      df = df.iloc[::-1]
      stock_df_list.append(df)
      Dates = np.array(df.index, int)
      current_Change = Dates[2] - Dates[1] 

      #update min and max date if new max or min is found
      if Min_date == None:
        Min_date = Dates[0]
      elif Dates[0] <= Min_date: 
        Min_date = Dates[0]
    
      if Dates[-1] >= Max_date:
        Max_date = Dates[-1]
      if Change == None:
        Change = current_Change
      elif current_Change < Change:
        Change = current_Change
      
    for df in stock_df_list:
      #tokeniser dates
      filtered_data = df.filter(['Open', 'High', 'Low', 'Close', 'Volume'])
      Dates = np.array(df.index, float)
      Scaled_Dates = Dates - Min_date
      Scaled_Dates /= Change
      Scaled_Dates = np.array(Scaled_Dates, int)
      Time_tokens.append(Scaled_Dates)

      scaled_data = filtered_data.pct_change()
      data = scaled_data.values[1:-1]
      Stock_data.append(data)
  
  X = np.array([])
  y = np.array([])
  for index, values in enumerate(Stock_tokens):
    Stock_df = len(Stock_data[index]) - 1
    for row_index in range(0, Stock_df):
      inp = np.array([Stock_tokens[index], Time_tokens[index][row_index]]) #, Stock_data[index][row_index][0], Stock_data[index][row_index][1], Stock_data[index][row_index][2], Stock_data[index][row_index][3], Stock_data[index][row_index][4]])
      X = np.append(X, inp)

      tar = np.array(Stock_data[index][row_index])
      y = np.append(y, tar)
  X = X.reshape((-1, 2))
  y = y.reshape((-1, 5))
  # y = tf.Tensor(y)
  Min_date = 0
  Max_date /= Change
  N_Tokens = int(Max_date - Min_date)
  return X, y, N_Tokens, failed_index

X, y, N_Tokens, failed_index = stock_clean(Stock_list)
print(N_Tokens)
print((X.shape, y.shape))

#save train data
with open('data.pkl', 'wb+') as file:
  pickle.dump((X, y, N_Tokens), file)

# temp = X.transpose()
# X_tokens = temp[:2]
# print(X_tokens.shape)

X = X.transpose()

Stock = tf.keras.layers.Input(name = 'Stock_inp', shape=[1])
Time = tf.keras.layers.Input(name = 'Time_inp', shape=[1])
# Data = tf.keras.layers.Input(name = 'Data_inp', shape=[5])

Stock_embedding = tf.keras.layers.Embedding(name = 'Stock_embedding', input_dim=Stock_len, output_dim=embedding_size)(Stock)
Time_embedding = tf.keras.layers.Embedding(name = 'Time_embedding', input_dim=N_Tokens, output_dim=embedding_size)(Time)
merged = tf.keras.layers.Dot(name = 'dot_product', normalize=True, axes=2)([Stock_embedding, Time_embedding])
output = tf.keras.layers.Dense(5)(merged)

model = tf.keras.Model(inputs = [Stock, Time], outputs = output)
model.compile(optimizer = 'Adam', loss = 'mse')
model.summary()
model.fit((X[0], X[1]), y, epochs=100)


model.save('models/sp500-100epochs2.h5')

Stock_layer = model.get_layer('Stock_embedding')
Stock_weights = Stock_layer.get_weights()[0]
Time_layer = model.get_layer('Time_embedding')
Time_weights = Time_layer.get_weights()[0]

Stock_weights.shape, Time_weights.shape

Stock_weights = Stock_weights / np.linalg.norm(Stock_weights, axis = 1).reshape((-1, 1))
Stock_weights[0][:10]

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline



def find_similar(name, weights, index_name = 'stock', n = 10, least = False, return_dist = False, plot = False):
    """Find n most similar items (or least) to name based on embeddings. Option to also plot the results"""
    # Select index and reverse index
    if index_name == 'stock':
        index = Stock_index
        rindex = Stock_list
    elif index_name == 'time':
        index = link_index
        rindex = index_link
    
    # Check to make sure `name` is in index
    try:
        # Calculate dot product between book and all others
        dists = np.dot(weights, weights[index[name]])
    except KeyError:
        print(f'{name} Not Found.')
        return
    
    # Sort distance indexes from smallest to largest
    sorted_dists = np.argsort(dists)
    
    # Plot results if specified
    if plot:
        
        # Find furthest and closest items
        furthest = sorted_dists[:(n // 2)]
        closest = sorted_dists[-n-1: len(dists) - 1]
        items = [rindex[c] for c in furthest]
        items.extend(rindex[c] for c in closest)
        
        # Find furthest and closets distances
        distances = [dists[c] for c in furthest]
        distances.extend(dists[c] for c in closest)
        
        colors = ['r' for _ in range(n //2)]
        colors.extend('g' for _ in range(n))
        
        data = pd.DataFrame({'distance': distances}, index = items)
        
        # Horizontal bar chart
        data['distance'].plot.barh(color = colors, figsize = (10, 8),
                                   edgecolor = 'k', linewidth = 2)
        plt.xlabel('Cosine Similarity');
        plt.axvline(x = 0, color = 'k');
        
        # Formatting for italicized title
        name_str = f'{index_name.capitalize()}s Most and Least Similar to'
        for word in name.split():
            # Title uses latex for italize
            name_str += ' $\it{' + word + '}$'
        plt.title(name_str, x = 0.2, size = 28, y = 1.05)
        
        return None
    
    # If specified, find the least similar
    if least:
        # Take the first n from sorted distances
        closest = sorted_dists[:n]
         
        print(f'{index_name.capitalize()}s furthest from {name}.\n')
        
    # Otherwise find the most similar
    else:
        # Take the last n sorted distances
        closest = sorted_dists[-n:]
        
        # Need distances later on
        if return_dist:
            return dists, closest
        
        
        print(f'{index_name.capitalize()}s closest to {name}.\n')
        
    # Need distances later on
    if return_dist:
        return dists, closest
    
    
    # Print formatting
    # max_width = max([len(rindex[c]) for c in closest])
    
    # Print the most similar and distances
    for c in reversed(closest):
        print(f'{index_name.capitalize()}: {rindex[c]} Similarity: {dists[c]:.{2}}')

find_similar('AMZN', Stock_weights, plot=True, n=10)


from sklearn.manifold import TSNE


def reduce_dim(weights, components = 3, method = 'tsne'):
    """Reduce dimensions of embeddings"""
    if method == 'tsne':
        return TSNE(components, metric = 'cosine').fit_transform(weights)

Stocks_r = reduce_dim(Stock_weights, components=2, method='tsne')
Stocks_r.shape

# InteractiveShell.ast_node_interactivity = 'last'

plt.figure(figsize = (10, 8))
plt.plot(Stocks_r[:, 0], Stocks_r[:, 1], 'r.')
plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('Stock Embeddings Visualized with TSNE')

from collections import Counter, OrderedDict

def count_items(l):
    """Return ordered dictionary of counts of objects in `l`"""
    
    # Create a counter object
    counts = Counter(l)
    
    # Sort by highest count first and place in ordered dictionary
    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)
    counts = OrderedDict(counts)
    
    return counts

Stocks_r.shape,

# get sectors
sect = table[0]['GICS Sector']
sect_counts = count_items(sect)
sect_summary = list(sect_counts.keys())
sect_summary

"""
idx_include = []
sects = []

for i, sector in enumerate(sects):
  if i not in failed_index:
    pass
  else:
    idx_include.append(i)
    sects.append(sector.capitalize())

"""
idx_include = []
sects = []

for i, sector in enumerate(sect):
  idx_include.append(i)
  sects.append(sector.capitalize())

ints, gen = pd.factorize(sects)
ints.shape

plt.figure(figsize = (10, 8))

# Plot embedding
plt.scatter(Stocks_r[idx_include, 0], Stocks_r[idx_include, 1], 
            c = ints, cmap = plt.cm.tab20)

# Add colorbar and appropriate labels
cbar = plt.colorbar()
cbar.set_ticks([])
for j, lab in enumerate(gen):
    cbar.ax.text(11, (10 * j + 94) / ((10) * 2), lab, ha='left', va='center')
cbar.ax.set_title('Stock', loc = 'left')


plt.xlabel('TSNE 1'); plt.ylabel('TSNE 2'); plt.title('TSNE Visualization of Stock Embeddings');